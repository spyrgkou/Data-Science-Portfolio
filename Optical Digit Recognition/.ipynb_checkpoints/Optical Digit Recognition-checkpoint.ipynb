{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db47788f",
   "metadata": {},
   "source": [
    "# Optical Digit Recognition\n",
    "In this notebook, we will develop and compare some classifiers for recognizing digits from images. More specifically, we will develop Gaussian Naive Bayes classifier from scratch and compare it with other classifiers. The data comes from the US Postal Service (handwritten on postal envelopes and scannes and available in Kaggle) and contains the digits 0 to 9.  First, we import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94574c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009289d5",
   "metadata": {},
   "source": [
    "We will use two files, one containing train data and the other containing test data. The data of each file represents the contents of an array with the values of the array being space-separated. Each line corresponds for one digit - sample. The columns correspond to the features that describe the digits. Each digit is described by 257 values of with the first corresponds to the digit itself and the remaining 256 are the features that describe it in greyscale values. Each digit is represented in a 16x16 matrix consisting of 256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 7291\n",
    "N_test = 2007\n",
    "n_features = 256\n",
    "X_train = np.zeros((N_train, n_features),dtype = np.float64)\n",
    "X_test = np.zeros((N_test, n_features), dtype = np.float64)\n",
    "y_train = np.zeros(N_train, dtype=np.int8)\n",
    "y_test = np.zeros(N_test, dtype=np.int8)\n",
    "\n",
    "with open(\"train.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.split()\n",
    "        y_train[i] = int(float(line[0]))\n",
    "        X_train[i] = np.asarray(line[1:])\n",
    "\n",
    "with open(\"test.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.split()\n",
    "        y_test[i]=int(float(line[0]))\n",
    "        X_test[i]=np.asarray(line[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3198b9",
   "metadata": {},
   "source": [
    "Then we define some utility functions that will help us develop our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(X, index):\n",
    "    '''Takes a dataset (e.g. X_train) and imshows the digit at the corresponding index\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        index (int): index of digit to show\n",
    "    '''\n",
    "    plt.imshow(X[index].reshape(16,16), cmap=\"gray\")\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "def plot_digits_samples(X, y):\n",
    "    '''Takes a dataset and selects one example from each label and plots it in subplots\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(18,8))\n",
    "    classes = list(np.unique(y))\n",
    "    for i in range(len(classes)):\n",
    "        el_imgs = np.where(y == classes[i])[0]\n",
    "        random_index = random.choice(el_imgs)\n",
    "        # random_img = X[random_index].reshape(16,16)\n",
    "        fig.add_subplot(2,5, classes[i]+1)\n",
    "        plt.axis('off')\n",
    "        show_sample(X, random_index)\n",
    "        plt.title(\"index: \"+str(random_index))\n",
    "\n",
    "        \n",
    "def digit_mean_at_pixel(X, y, digit, pixel=(10, 10)):\n",
    "    '''Calculates the mean for all instances of a specific digit at a pixel location\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "        digit (int): The digit we need to select\n",
    "        pixels (tuple of ints): The pixels we need to select.\n",
    "    Returns:\n",
    "        (float): The mean value of the digits for the specified pixels\n",
    "    '''\n",
    "    indices = np.where(y==digit)\n",
    "    digit_imgs = X[indices]\n",
    "    reshaped_imgs = np.reshape(digit_imgs, (digit_imgs.shape[0], 16, 16))\n",
    "    pixel_imgs = reshaped_imgs[:,pixel[0],pixel[1]]\n",
    "    return np.mean(pixel_imgs)\n",
    "\n",
    "def digit_variance_at_pixel(X, y, digit, pixel=(10, 10)):\n",
    "    '''Calculates the variance for all instances of a specific digit at a pixel location\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "        digit (int): The digit we need to select\n",
    "        pixels (tuple of ints): The pixels we need to select\n",
    "    Returns:\n",
    "        (float): The variance value of the digits for the specified pixels\n",
    "    '''\n",
    "    indices = np.where(y==digit)\n",
    "    digit_imgs = X[indices]\n",
    "    reshaped_imgs = np.reshape(digit_imgs, (digit_imgs.shape[0], 16, 16))\n",
    "    pixel_imgs = reshaped_imgs[:,pixel[0],pixel[1]]\n",
    "    return np.var(pixel_imgs)\n",
    "\n",
    "def digit_mean(X, y, digit):\n",
    "    '''Calculates the mean for all instances of a specific digit\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "        digit (int): The digit we need to select\n",
    "    Returns:\n",
    "        (np.ndarray): The mean value of the digits for every pixel\n",
    "    '''\n",
    "    indices = np.where(y==digit)\n",
    "    digit_imgs = X[indices]\n",
    "    return np.mean(digit_imgs, axis=0)\n",
    "\n",
    "def digit_variance(X, y, digit):\n",
    "    '''Calculates the variance for all instances of a specific digit\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "        digit (int): The digit we need to select\n",
    "    Returns:\n",
    "        (np.ndarray): The variance value of the digits for every pixel\n",
    "    '''\n",
    "    indices = np.where(y==digit)\n",
    "    digit_imgs = X[indices]\n",
    "    return np.var(digit_imgs, axis=0)\n",
    "\n",
    "def calculate_priors(X, y):\n",
    "    \"\"\"Return the a-priori probabilities for every class\n",
    "    Args:\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "    Returns:\n",
    "        (np.ndarray): (n_classes) Prior probabilities for every class\n",
    "    \"\"\"\n",
    "    return np.bincount(y)/len(y)\n",
    "\n",
    "pdf = lambda x,m,s: np.sqrt(1/(2*np.pi*(s+10**(-7)))) * np.exp(-((x-m)**2)/(2*s+10**(-7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ca88d",
   "metadata": {},
   "source": [
    "Next we randomly pick a digit from every class and plot it in a figure with subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_samples(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aed405",
   "metadata": {},
   "source": [
    "Moreover we have developed functions in order to develop image statistics like the mean value and the variance of a specific pixel or of a specific digit, as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0 = digit_mean_at_pixel(X_train, y_train, 0, pixel=(10,10))\n",
    "print(\"The mean value of pixel (10,10) of 0's in the train dataset is: {:.5f}\".format(mean_0))\n",
    "var_0 = digit_variance_at_pixel(X_train, y_train, 0, pixel=(10, 10))\n",
    "print(\"The variance of pixel (10,10) of 0's in the train dataset is: {:.5f}\".format(var_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c7c6c",
   "metadata": {},
   "source": [
    "We compute the mean value images of all digits and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_digits = np.zeros((10,n_features), dtype=np.float64)\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "for i in range(0,10):\n",
    "    mean_digits[i] = digit_mean(X_train, y_train, i)\n",
    "    fig.add_subplot(2,5,i+1)\n",
    "    plt.imshow(mean_digits[i].reshape(16,16), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Mean value of {} digit\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa238b7c",
   "metadata": {},
   "source": [
    "We compute the variance images of all digits and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92412af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_digits = np.zeros((10,n_features), dtype=np.float64)\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "for i in range(0,10):\n",
    "    var_digits[i] = digit_variance(X_train, y_train, i)\n",
    "    fig.add_subplot(2,5,i+1)\n",
    "    plt.imshow(var_digits[i].reshape(16,16), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Variance of {} digit\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035adfca",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "Next we implement the Gaussian Naive Bayes Classifier from scratch. The classifier is compatible with scikit-learn and it is build upon its base estimator. The basic intuition of the classifier is the computation the gaussian probability density function of each class, and then the classification of each sample to the class the has the highest probability. Considering the Bayes Theorem, we have:\n",
    "\n",
    "$$P(y| x_1, ..., x_n)=\\frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}\n",
    "= \\frac{P(y) {\\prod_{i=1}^{n}P(x_i|y)}}{P(x_1,...,x_n)}$$\n",
    "\n",
    "$P(y)$ are the a-priori probabilities of each class and are computed as the relative frequency from the train data. Considering that the features (pixels) are statistically independent and they correlate only to their relevant class:\n",
    "$$P(x_i|y, x_1,...,x_{i-1},x_{i+1},...,x_n) = P(x_i,y)$$\n",
    "The classification rule goes as:\n",
    "$$\\hat{y} = argmax_yP(y){\\prod_{i=1}^{n}P(x_i|y)}$$\n",
    "Considering\n",
    "$$P(x_i|y) = \\frac{1}{{\\sigma_y \\sqrt {2\\pi } }}e^{{{( {x_i - \\mu_y })^2 }{{{{-( {x_i - \\mu_y })^2 }{2\\sigma_y ^2 }}}} {2\\sigma_y ^2 }}} $$\n",
    "The estimation of each class a given feature is the one that maximizes the above probability. We have to consider that the only parameter of our implementation is the variance of the features. We can use unit variance (equal to 1 for all the classes), and not the variance corresponding to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Custom implementation Naive Bayes classifier\"\"\"\n",
    "\n",
    "    def __init__(self, use_unit_variance=False):\n",
    "        self.use_unit_variance = use_unit_variance\n",
    "        self.X_mean_ = None\n",
    "        self.X_var_ = None\n",
    "        self.apriori = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This should fit classifier. All the \"work\" should be done here.\n",
    "        Calculates self.X_mean_ and self.X_var_ based on the mean feature values in X for each class\n",
    "        self.X_mean_ becomes a numpy.ndarray of shape(n_classes, n_features)\n",
    "        self.X_var_ becomes a numpy.ndarray of shape(n_classes, n_features) considering if use_unit_variance is true\n",
    "        self.apriori_ becomes a numpy.ndarray of shape(n_classes, n_features)\n",
    "        Calculate the prior posibilities for the classes\n",
    "        self.apriori becomes numpy.ndarray of shape(n_classes, 1)\n",
    "        fit always returns self.\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.X_mean_ = np.zeros((len(self.classes), X.shape[1]), dtype = np.float64)\n",
    "        self.X_var_ = np.zeros((len(self.classes), X.shape[1]), dtype = np.float64)\n",
    "        for digit in self.classes:\n",
    "            self.X_mean_[digit] = digit_mean(X, y, digit)\n",
    "            if not self.use_unit_variance:\n",
    "                self.X_var_[digit] = digit_variance(X, y, digit)\n",
    "            else:\n",
    "                self.X_var_[digit] =1\n",
    "            \n",
    "        self.apriori = calculate_priors(X,y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for X based on the\n",
    "        euclidean distance from self.X_mean_\n",
    "        \"\"\"\n",
    "#         aposteriori = np.asarray([np.prod(pdf(X, self.X_mean_[digit], self.X_var_[digit]),axis=1) \\\n",
    "#                                     for digit in self.classes])\n",
    "        aposteriori = np.asarray([np.sum(np.log(pdf(X, self.X_mean_[digit], self.X_var_[digit])),axis=1) \\\n",
    "                                    for digit in self.classes])\n",
    "#         return np.argmax(aposteriori.T*self.apriori, axis=1)\n",
    "        return np.argmax(aposteriori.T + self.apriori, axis=1)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return accuracy score on the predictions\n",
    "        for X based on ground truth y\n",
    "        \"\"\"\n",
    "        predicted = self.predict(X)\n",
    "        return (np.sum(predicted==y))/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d277",
   "metadata": {},
   "source": [
    "Next we train the classifier using our training data and compute the accuracy of our model using the test data. We have created two models, the first using unit variance and the second without it and the corresponding accuracies on the test data were 0.8137 and 0.7309."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1705d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB_clf = CustomNBClassifier(use_unit_variance=False)\n",
    "GNB_clf.fit(X_train, y_train)\n",
    "GNB_clf_score = GNB_clf.score(X_test, y_test)\n",
    "print(\"The accuracy of the GNB classifier using the data variance is {:.4f}\".format(GNB_clf_score))\n",
    "\n",
    "GNB_clf_var = CustomNBClassifier(use_unit_variance=True)\n",
    "GNB_clf_var.fit(X_train, y_train)\n",
    "GNB_clf_var_score = GNB_clf_var.score(X_test, y_test)\n",
    "print(\"The accuracy of the GNB classifier using unit variance is {:.4f}\".format(GNB_clf_var_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676fafe",
   "metadata": {},
   "source": [
    "We will compare our classifier with some scikit-learn classifiers like Gaussian Naive Bayes, k-NN and SVM using polynomial kernel. For the training of our models we will use 5-fold Cross Validation. Cross Validation is a resampling method that uses different portions of the data to test and train a model on different iterations, and one wants to estimate the model's accuracy in practice. Next we define the classifiers and the functions used for their training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cec506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, X, y, folds=5):\n",
    "    \"\"\"Returns the 5-fold accuracy for classifier clf on X and y\n",
    "    Args:\n",
    "        clf (sklearn.base.BaseEstimator): classifier\n",
    "        X (np.ndarray): Digits data (nsamples x nfeatures)\n",
    "        y (np.ndarray): Labels for dataset (nsamples)\n",
    "    Returns:\n",
    "        (float): The 5-fold classification score (accuracy)\n",
    "    \"\"\"\n",
    "    clf_scores = cross_val_score(clf, X, y, cv=folds)\n",
    "    return clf_scores\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "skNB_classifier = GaussianNB()\n",
    "customNB_classifier = CustomNBClassifier(use_unit_variance=True)\n",
    "svc_poly = SVC(kernel=\"poly\")\n",
    "\n",
    "kNNscores = evaluate_classifier(knn_classifier, X_train, y_train, folds=5)\n",
    "skNBscores = evaluate_classifier(skNB_classifier, X_train, y_train, folds=5)\n",
    "customNBscores = evaluate_classifier(customNB_classifier, X_train, y_train, folds=5)\n",
    "svm_scores = evaluate_classifier(svc_poly, X_train, y_train, folds=5)\n",
    "\n",
    "compare_scores= {\n",
    "    \"knn\" : np.mean(kNNscores),\n",
    "    \"scikit GNB\" : np.mean(skNBscores),\n",
    "    \"custom GNB\" : np.mean(customNBscores),\n",
    "    \"SVM (polynomial kernel)\" : np.mean(svm_scores)\n",
    "}\n",
    "\n",
    "for key in compare_scores:\n",
    "    print(\"The accuracy of \"+key+\" classifier is {:.5f}\".format(compare_scores[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90b805",
   "metadata": {},
   "source": [
    "We will use ensembling techniques to combine the classifiers with the best accuracy score in order to increase the accuracy of them by comparing those to each other. First we will use the Voting Classifier, which computes the mean value of predictions of all the classifiers involved. From a frequency perspective, the classification error is distributed to the voting classifier considering the tradeoff between voting and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7809d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "allestimators = [(\"SVM (polynomial kernel)\", svc_poly), (\"knn\", knn_classifier),(\"custom GNB\", customNB_classifier)]\n",
    "voting_scores = evaluate_classifier(VotingClassifier(allestimators, voting = 'hard'), X_train, y_train, folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a97cbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"The Voting Classifier using SVM polynomial, kNN, custom Gauss Naive Bayes classifier(5-fold cv) has {:.4f}\"\\\n",
    "      \"+-{:.5f}\".format(np.mean(voting_scores),np.std(voting_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02e0e5",
   "metadata": {},
   "source": [
    "Another ensembling technique is Bagging classifier. The difference between Bagging and Voting classifier is that during the training phase, the classifiers are trained in sequence and each classifier is trained using a weighed part of training data and the weight of each feature is relevant to the accuracy of the previous classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d98fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_scores= {\n",
    "    \"knn\" : evaluate_classifier(BaggingClassifier(knn_classifier, n_estimators=10, random_state=0), \n",
    "                                X_train, y_train, folds=5),\n",
    "    \"custom GNB\" : evaluate_classifier(BaggingClassifier(customNB_classifier, n_estimators=10, random_state=0) , \n",
    "                                       X_train, y_train, folds=5),\n",
    "    \"SVM (polynomial kernel)\" : evaluate_classifier(BaggingClassifier(svc_poly, n_estimators=10, random_state=0), \n",
    "                                                    X_train, y_train, folds=5),\n",
    "}\n",
    "\n",
    "for key in bagging_scores:\n",
    "    print(\"The accuracy for \"+key+\" classifier is {:.4f}\".format(np.mean(bagging_scores[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalDigitRecognitionDataset(Dataset):\n",
    "    \"\"\"OpticalDigitRecognitionDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: feature in np.array\n",
    "            y: labels in np.array\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_classes = np.unique(self.y).shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            return self.transform((self.X[idx], self.y[idx]))\n",
    "        else:\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, datum):\n",
    "        x, y = datum[0], datum[1]\n",
    "        t = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        return t, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "to_tensor_trasform = transforms.Compose([ToTensor()])\n",
    "\n",
    "nn_train_data = OpticalDigitRecognitionDataset(X_train, y_train, transform= to_tensor_trasform)\n",
    "nn_train_test = OpticalDigitRecognitionDataset(X_test, y_test, transform= to_tensor_trasform)\n",
    "\n",
    "train_loader = DataLoader(nn_train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(nn_train_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3layers(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, H_1, H_2, n_classes):\n",
    "        super(NN3layers, self).__init__()\n",
    "        self.l1 = nn.Linear(n_features, H_1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(H_1, H_2)\n",
    "        self.l3 = nn.Linear(H_2, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.l1(x))\n",
    "        h2 = self.relu(self.l2(h1))\n",
    "        h = self.l3(h2)\n",
    "        return F.softmax(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3sklearn(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_features, H_1, H_2, n_classes, epochs, batch_size):\n",
    "        self.n_features = n_features\n",
    "        self.H_1 = H_1\n",
    "        self.H_2 = H_2\n",
    "        self.n_classes = n_classes\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = NN3layers(n_features = n_features, H_1 = self.H_1, H_2 = self.H_2, n_classes=n_classes)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.02, momentum = 0.95)\n",
    "        to_tensor_trasform = transforms.Compose([ToTensor()])\n",
    "        train_data = OpticalDigitRecognitionDataset(X, y, transform = to_tensor_trasform)\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        for epoch in range(self.epochs):\n",
    "            for i, data in enumerate(train_loader):\n",
    "                X_batch, y_batch = data\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch.type(torch.LongTensor))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        to_tensor_trasform = transforms.Compose([ToTensor()])\n",
    "        test_dataset = OpticalDigitRecognitionDataset(X, y, transform = to_tensor_trasform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        self.model.eval() \n",
    "        predictions = []\n",
    "        correct = 0\n",
    "        ns = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                X_batch, y_batch = data\n",
    "                out = self.model(X_batch)\n",
    "                val, y_pred = out.max(1)\n",
    "                predictions.append(y_pred[0].item())\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predicted = self.predict(X, y)\n",
    "        return (np.sum(predicted==y))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144efd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = nn_train_data.n_features\n",
    "n_classes = nn_train_data.n_classes\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "H_1 = 32\n",
    "H_2 = 64\n",
    "\n",
    "NNsklearnModel = NN3sklearn( n_features=n_features, H_1=H_1, H_2=H_2, n_classes=n_classes, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a726c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNsklearnModel.fit(X_train, y_train)\n",
    "print(\"The accuracy for the training data is: {0:.4f}\".format(NNsklearnModel.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1345c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNsklearnModel.fit(X_test, y_test)\n",
    "print(\"The accuracy for the testing data is: {0:.4f}\".format(NNsklearnModel.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsportfolio] *",
   "language": "python",
   "name": "conda-env-dsportfolio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
